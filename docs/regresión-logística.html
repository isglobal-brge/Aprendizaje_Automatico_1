<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Regresión logística | Aprendizaje Automático 1</title>
  <meta name="description" content="6 Regresión logística | Aprendizaje Automático 1" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Regresión logística | Aprendizaje Automático 1" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Regresión logística | Aprendizaje Automático 1" />
  
  
  

<meta name="author" content="Juan R González" />


<meta name="date" content="2020-10-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ajuste-de-modelos.html"/>

<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Aprendizaje Automático 1</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html"><i class="fa fa-check"></i><b>2</b> Introducción a Tidyverse</a>
<ul>
<li class="chapter" data-level="2.1" data-path="index.html"><a href="index.html#introducción"><i class="fa fa-check"></i><b>2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#instalación"><i class="fa fa-check"></i><b>2.2</b> Instalación</a></li>
<li class="chapter" data-level="2.3" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#librerías-básicas"><i class="fa fa-check"></i><b>2.3</b> Librerías básicas</a></li>
<li class="chapter" data-level="2.4" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#manejo-de-datos"><i class="fa fa-check"></i><b>2.4</b> Manejo de datos</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#tibbles"><i class="fa fa-check"></i><b>2.4.1</b> Tibbles</a></li>
<li class="chapter" data-level="2.4.2" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#importar-datos"><i class="fa fa-check"></i><b>2.4.2</b> Importar datos</a></li>
<li class="chapter" data-level="2.4.3" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#transformación-de-datos"><i class="fa fa-check"></i><b>2.4.3</b> Transformación de datos</a></li>
<li class="chapter" data-level="2.4.4" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#filtrar-filas"><i class="fa fa-check"></i><b>2.4.4</b> Filtrar filas</a></li>
<li class="chapter" data-level="2.4.5" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#filtrado-lógico"><i class="fa fa-check"></i><b>2.4.5</b> Filtrado lógico</a></li>
<li class="chapter" data-level="2.4.6" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#ordenar-filas"><i class="fa fa-check"></i><b>2.4.6</b> Ordenar filas</a></li>
<li class="chapter" data-level="2.4.7" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#seleccionar-columnas-e.g.-variables"><i class="fa fa-check"></i><b>2.4.7</b> Seleccionar columnas (e.g. variables)</a></li>
<li class="chapter" data-level="2.4.8" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#añadir-nuevas-variables"><i class="fa fa-check"></i><b>2.4.8</b> Añadir nuevas variables</a></li>
<li class="chapter" data-level="2.4.9" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#grouped-summaries"><i class="fa fa-check"></i><b>2.4.9</b> Grouped summaries</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#uso-del-pipe"><i class="fa fa-check"></i><b>2.5</b> Uso del pipe <code>%&gt;%</code></a></li>
<li class="chapter" data-level="2.6" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#ejercicios-manejo-de-datos"><i class="fa fa-check"></i><b>2.6</b> Ejercicios (manejo de datos)</a></li>
<li class="chapter" data-level="2.7" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#visualización-de-datos"><i class="fa fa-check"></i><b>2.7</b> Visualización de datos</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#distribución-de-datos-categóricos"><i class="fa fa-check"></i><b>2.7.1</b> Distribución de datos categóricos</a></li>
<li class="chapter" data-level="2.7.2" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#distribución-de-datos-continuos"><i class="fa fa-check"></i><b>2.7.2</b> Distribución de datos continuos</a></li>
<li class="chapter" data-level="2.7.3" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#distribución-de-datos-continuos-según-una-variable-categórica"><i class="fa fa-check"></i><b>2.7.3</b> Distribución de datos continuos según una variable categórica</a></li>
<li class="chapter" data-level="2.7.4" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#dos-variables-categóricas"><i class="fa fa-check"></i><b>2.7.4</b> Dos variables categóricas</a></li>
<li class="chapter" data-level="2.7.5" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#dos-variables-continuas"><i class="fa fa-check"></i><b>2.7.5</b> Dos variables continuas</a></li>
<li class="chapter" data-level="2.7.6" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#facets"><i class="fa fa-check"></i><b>2.7.6</b> Facets</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="introducción-a-tidyverse.html"><a href="introducción-a-tidyverse.html#ejercicios-visualización-de-datos"><i class="fa fa-check"></i><b>2.8</b> Ejercicios (Visualización de datos)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html"><i class="fa fa-check"></i><b>3</b> Introducción al Aprendizaje Automático</a></li>
<li class="chapter" data-level="4" data-path="regresión-lineal.html"><a href="regresión-lineal.html"><i class="fa fa-check"></i><b>4</b> Regresión lineal</a>
<ul>
<li class="chapter" data-level="4.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#preliminares"><i class="fa fa-check"></i><b>4.1</b> Preliminares</a></li>
<li class="chapter" data-level="4.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#conceptos-básicos"><i class="fa fa-check"></i><b>4.2</b> Conceptos básicos</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#modelo-lineal-simple"><i class="fa fa-check"></i><b>4.2.1</b> Modelo lineal simple</a></li>
<li class="chapter" data-level="4.2.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#regresión-lineal-multivariante"><i class="fa fa-check"></i><b>4.2.2</b> Regresión lineal multivariante</a></li>
<li class="chapter" data-level="4.2.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#incertidumbre"><i class="fa fa-check"></i><b>4.2.3</b> Incertidumbre</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#ajuste-de-un-modelo-lineal"><i class="fa fa-check"></i><b>4.3</b> Ajuste de un modelo lineal</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#residuos"><i class="fa fa-check"></i><b>4.3.1</b> Residuos</a></li>
<li class="chapter" data-level="4.3.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#coeficientes-de-interpretación"><i class="fa fa-check"></i><b>4.3.2</b> Coeficientes de interpretación</a></li>
<li class="chapter" data-level="4.3.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#interacciones"><i class="fa fa-check"></i><b>4.3.3</b> Interacciones</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="regresión-lineal.html"><a href="regresión-lineal.html#estimación-por-mínimos-cuadrados"><i class="fa fa-check"></i><b>4.4</b> Estimación por mínimos cuadrados</a></li>
<li class="chapter" data-level="4.5" data-path="regresión-lineal.html"><a href="regresión-lineal.html#medidas-adicionales-de-ajuste-del-modelo"><i class="fa fa-check"></i><b>4.5</b> Medidas adicionales de ajuste del modelo</a></li>
<li class="chapter" data-level="4.6" data-path="regresión-lineal.html"><a href="regresión-lineal.html#sesgo-variación-sobreajuste"><i class="fa fa-check"></i><b>4.6</b> Sesgo, variación, sobreajuste</a></li>
<li class="chapter" data-level="4.7" data-path="regresión-lineal.html"><a href="regresión-lineal.html#regresión-como-estimación-de-una-media-condicional"><i class="fa fa-check"></i><b>4.7</b> Regresión como estimación de una media condicional</a></li>
<li class="chapter" data-level="4.8" data-path="regresión-lineal.html"><a href="regresión-lineal.html#la-función-de-regresión"><i class="fa fa-check"></i><b>4.8</b> La función de regresión</a></li>
<li class="chapter" data-level="4.9" data-path="regresión-lineal.html"><a href="regresión-lineal.html#estimación-no-paramétrica-de-la-función-de-regresión-regresión-knn"><i class="fa fa-check"></i><b>4.9</b> Estimación no paramétrica de la función de regresión: regresión KNN</a></li>
<li class="chapter" data-level="4.10" data-path="regresión-lineal.html"><a href="regresión-lineal.html#estimación-paramétrica-de-la-función-de-regresión-regresión-lineal"><i class="fa fa-check"></i><b>4.10</b> Estimación paramétrica de la función de regresión: regresión lineal</a></li>
<li class="chapter" data-level="4.11" data-path="regresión-lineal.html"><a href="regresión-lineal.html#predicción"><i class="fa fa-check"></i><b>4.11</b> Predicción</a></li>
<li class="chapter" data-level="4.12" data-path="regresión-lineal.html"><a href="regresión-lineal.html#inferencia-en-el-contexto-de-regresión"><i class="fa fa-check"></i><b>4.12</b> Inferencia en el contexto de regresión</a></li>
<li class="chapter" data-level="4.13" data-path="regresión-lineal.html"><a href="regresión-lineal.html#asunciones-de-un-modelo-de-regresión"><i class="fa fa-check"></i><b>4.13</b> Asunciones de un modelo de regresión</a></li>
<li class="chapter" data-level="4.14" data-path="regresión-lineal.html"><a href="regresión-lineal.html#ejemplos-adicionales-de-interpretación-de-modelos"><i class="fa fa-check"></i><b>4.14</b> Ejemplos adicionales de interpretación de modelos</a>
<ul>
<li class="chapter" data-level="4.14.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#interpretación-del-intercept-y-los-coeficientes-beta-para-un-modelo-con-predictores-continuos"><i class="fa fa-check"></i><b>4.14.1</b> Interpretación del <em>intercept</em> y los coeficientes <span class="math inline">\(\beta\)</span> para un modelo con predictores continuos</a></li>
<li class="chapter" data-level="4.14.2" data-path="regresión-lineal.html"><a href="regresión-lineal.html#interpretación-del-intercept-y-los-coeficientes-beta-para-un-modelo-con-predictores-binarios-y-continuos"><i class="fa fa-check"></i><b>4.14.2</b> Interpretación del <em>intercept</em> y los coeficientes <span class="math inline">\(\beta\)</span> para un modelo con predictores binarios y continuos</a></li>
<li class="chapter" data-level="4.14.3" data-path="regresión-lineal.html"><a href="regresión-lineal.html#interpretación-del-intercept-y-los-coeficientes-beta-para-un-modelo-con-predictores-binarios-y-continuos-con-interacciones"><i class="fa fa-check"></i><b>4.14.3</b> Interpretación del <em>intercept</em> y los coeficientes <span class="math inline">\(\beta\)</span> para un modelo con predictores binarios y continuos, con interacciones</a></li>
<li class="chapter" data-level="4.14.4" data-path="regresión-lineal.html"><a href="regresión-lineal.html#interpretación-del-intercept-y-los-coeficientes-beta-para-un-modelo-con-predictores-continuos-con-interacciones"><i class="fa fa-check"></i><b>4.14.4</b> Interpretación del <em>intercept</em> y los coeficientes <span class="math inline">\(\beta\)</span> para un modelo con predictores continuos, con interacciones</a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="regresión-lineal.html"><a href="regresión-lineal.html#centrado-y-escalado"><i class="fa fa-check"></i><b>4.15</b> Centrado y escalado</a></li>
<li class="chapter" data-level="4.16" data-path="regresión-lineal.html"><a href="regresión-lineal.html#transformación-de-variables"><i class="fa fa-check"></i><b>4.16</b> Transformación de variables</a></li>
<li class="chapter" data-level="4.17" data-path="regresión-lineal.html"><a href="regresión-lineal.html#colinealidad"><i class="fa fa-check"></i><b>4.17</b> Colinealidad</a></li>
<li class="chapter" data-level="4.18" data-path="regresión-lineal.html"><a href="regresión-lineal.html#valores-atípicos"><i class="fa fa-check"></i><b>4.18</b> Valores atípicos</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ajuste-de-modelos.html"><a href="ajuste-de-modelos.html"><i class="fa fa-check"></i><b>5</b> Ajuste de modelos</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ajuste-de-modelos.html"><a href="ajuste-de-modelos.html#reglas-generales-para-la-selección-de-variables"><i class="fa fa-check"></i><b>5.1</b> Reglas generales para la selección de variables</a></li>
<li class="chapter" data-level="5.2" data-path="ajuste-de-modelos.html"><a href="ajuste-de-modelos.html#selección-paso-a-paso-stepwise"><i class="fa fa-check"></i><b>5.2</b> Selección paso a paso (stepwise)</a></li>
<li class="chapter" data-level="5.3" data-path="ajuste-de-modelos.html"><a href="ajuste-de-modelos.html#comparación-de-modelos"><i class="fa fa-check"></i><b>5.3</b> Comparación de modelos</a></li>
<li class="chapter" data-level="5.4" data-path="ajuste-de-modelos.html"><a href="ajuste-de-modelos.html#métodos-de-selección-automática"><i class="fa fa-check"></i><b>5.4</b> Métodos de selección automática</a></li>
<li class="chapter" data-level="5.5" data-path="ajuste-de-modelos.html"><a href="ajuste-de-modelos.html#validación-cruzada"><i class="fa fa-check"></i><b>5.5</b> Validación cruzada</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="ajuste-de-modelos.html"><a href="ajuste-de-modelos.html#validación-en-un-conjunto-de-datos-externo"><i class="fa fa-check"></i><b>5.5.1</b> Validación en un conjunto de datos externo</a></li>
<li class="chapter" data-level="5.5.2" data-path="ajuste-de-modelos.html"><a href="ajuste-de-modelos.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>5.5.2</b> Leave-one-out cross validation (LOOCV)</a></li>
<li class="chapter" data-level="5.5.3" data-path="ajuste-de-modelos.html"><a href="ajuste-de-modelos.html#k-fold-cross-validation-k-fold-cv"><i class="fa fa-check"></i><b>5.5.3</b> K-fold cross validation (K-fold CV)</a></li>
<li class="chapter" data-level="5.5.4" data-path="ajuste-de-modelos.html"><a href="ajuste-de-modelos.html#uso-de-cv-para-estimar-el-hiper-parámetro"><i class="fa fa-check"></i><b>5.5.4</b> Uso de CV para estimar el hiper-parámetro</a></li>
<li class="chapter" data-level="5.5.5" data-path="ajuste-de-modelos.html"><a href="ajuste-de-modelos.html#uso-de-bootstrap"><i class="fa fa-check"></i><b>5.5.5</b> Uso de bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="ajuste-de-modelos.html"><a href="ajuste-de-modelos.html#imputación-de-datos-faltantes"><i class="fa fa-check"></i><b>5.6</b> Imputación de datos faltantes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regresión-logística.html"><a href="regresión-logística.html"><i class="fa fa-check"></i><b>6</b> Regresión logística</a>
<ul>
<li class="chapter" data-level="6.1" data-path="regresión-logística.html"><a href="regresión-logística.html#la-función-logit-inversa"><i class="fa fa-check"></i><b>6.1</b> La función logit inversa</a></li>
<li class="chapter" data-level="6.2" data-path="regresión-logística.html"><a href="regresión-logística.html#ejemplo-de-regresión-logística"><i class="fa fa-check"></i><b>6.2</b> Ejemplo de regresión logística</a></li>
<li class="chapter" data-level="6.3" data-path="regresión-logística.html"><a href="regresión-logística.html#coeficientes-de-regresión-logística-como-probabilidades"><i class="fa fa-check"></i><b>6.3</b> Coeficientes de regresión logística como probabilidades</a></li>
<li class="chapter" data-level="6.4" data-path="regresión-logística.html"><a href="regresión-logística.html#coeficientes-de-regresión-logística-como-razones-de-odds"><i class="fa fa-check"></i><b>6.4</b> Coeficientes de regresión logística como razones de odds</a></li>
<li class="chapter" data-level="6.5" data-path="regresión-logística.html"><a href="regresión-logística.html#bondad-de-ajuste"><i class="fa fa-check"></i><b>6.5</b> Bondad de ajuste</a></li>
<li class="chapter" data-level="6.6" data-path="regresión-logística.html"><a href="regresión-logística.html#ejemplo-de-regresión-logística-modelización-de-riesgo-diabetes"><i class="fa fa-check"></i><b>6.6</b> Ejemplo de regresión logística: modelización de riesgo diabetes</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="regresión-logística.html"><a href="regresión-logística.html#modelo-simple"><i class="fa fa-check"></i><b>6.6.1</b> Modelo simple</a></li>
<li class="chapter" data-level="6.6.2" data-path="regresión-logística.html"><a href="regresión-logística.html#agregar-predictores-y-evaluar-el-ajuste"><i class="fa fa-check"></i><b>6.6.2</b> Agregar predictores y evaluar el ajuste</a></li>
<li class="chapter" data-level="6.6.3" data-path="regresión-logística.html"><a href="regresión-logística.html#análisis-de-interacciones"><i class="fa fa-check"></i><b>6.6.3</b> Análisis de interacciones</a></li>
<li class="chapter" data-level="6.6.4" data-path="regresión-logística.html"><a href="regresión-logística.html#gráfico-de-la-interacción"><i class="fa fa-check"></i><b>6.6.4</b> Gráfico de la interacción</a></li>
<li class="chapter" data-level="6.6.5" data-path="regresión-logística.html"><a href="regresión-logística.html#uso-del-modelo-para-predecir-probabilidades"><i class="fa fa-check"></i><b>6.6.5</b> Uso del modelo para predecir probabilidades</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="regresión-logística.html"><a href="regresión-logística.html#creación-de-un-modelo-y-validación"><i class="fa fa-check"></i><b>6.7</b> Creación de un modelo y validación</a></li>
<li class="chapter" data-level="6.8" data-path="regresión-logística.html"><a href="regresión-logística.html#nomogramas"><i class="fa fa-check"></i><b>6.8</b> Nomogramas</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje Automático 1</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regresión-logística" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Regresión logística</h1>
<p>Este capítulo introduce la regresión logística como el método más sencillo para crear modelos predictivos en problemas de clasificación que es el principal objetivo de la asignatura. Se cubrirán los siguientes temas:</p>
<ul>
<li>Conocer la función logística</li>
<li>Cómo interpretar los coeficientes de los modelos</li>
<li>Cómo evaluar la bondad de ajuste</li>
<li>Cómo interpretar variables</li>
<li>Ilustrar un ejemplo de análisis completo</li>
<li>Aprender a hacer nomogramas fijos y dinámicos</li>
</ul>
<p>Hasta ahora, nuestra variable de resultado era continua. Pero si la variable de resultado es binaria (0/1, “No”/“Sí”), entonces nos enfrentamos a un problema de <em>clasificación</em>. El objetivo de la clasificación es crear un modelo capaz de <em>clasificar</em> el resultado — y, cuando se usa el modelo para la predicción, nuevas observaciones— en una de dos categorías. La regresión logística se introduce en el contexto de la epidemiología como un modelo de regresión que extiende el modelo lineal cuando nuestra variable respuesta es binaria, pero tambié es, probablemente, el método estadístico más utilizado para la clasificación y el más sencillo. Una de las grandes ventajas de estos modelos respectoa otros que veremo más adelante es que este método produce un <em>modelo de probabilidad</em> para nuestra variable resultado. En otras palabras, los valores ajustados en un modelo logístico o logit no son binarios sino que son <em>probabilidades</em> que representan la probabilidad de que el resultado pertenezca a una de nuestras dos categorías.</p>
<p>Desafortunadamente, debemos afrontar nuevas complicaciones cuando trabajamos con regresión logística, lo que hace que estos modelos sean inherentemente más difíciles de interpretar que los modelos lineales. Las complicaciones surgen del hecho de que con la regresión logística modelamos la probabilidad de que <span class="math inline">\(y\)</span> = 1, y la probabilidad siempre se escala entre 0 y 1. Pero el predictor lineal, <span class="math inline">\(X_i \beta\)</span>, oscila entre <span class="math inline">\(\pm \infty\)</span> (donde <span class="math inline">\(X\)</span> representa todos los predictores del modelo). Esta diferencia de escala requiere transformar la variable de resultado, lo cual se logra con la función logit:</p>
<p><span class="math display">\[
\text{logit}(x) = \text{log}\left( \frac{x}{1-x} \right)
\]</span></p>
<p>La función logit asigna el rango del resultado (0,1) al rango del predictor lineal <span class="math inline">\((-\infty, +\infty)\)</span>. El resultado transformado, <span class="math inline">\(\text{logit} (x)\)</span>, se expresa en logaritmos de probabilidades (<span class="math inline">\(\frac{x}{1-x}\)</span> se conoce como probabilidades del resultado - razón de odds en inglés - momios en castellano). Así que el modelo también se puede escribir como:</p>
<p><span class="math display">\[\text{Pr}(y_i = 1) = p_i\]</span></p>
<p><span class="math display">\[\text{logit}(p_i) =  X_i\beta\]</span></p>
<p>Las probabilidades logarítmicas (e.g. el log-odds) no tienen interpretación (que no sea el signo y la magnitud) y deben transformarse nuevamente en cantidades interpretables, ya sea en <em>probabilidades</em>, usando el logit inverso, o en <em>razones de probabilidades</em>, mediante el uso de la función exponencial. Discutimos ambas transformaciones a continuación.</p>
<div id="la-función-logit-inversa" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> La función logit inversa</h2>
<p>El modelo logístico se puede escribir, alternativamente, usando el logit inverso:</p>
<p><span class="math display">\[
\operatorname{Pr}(y_i = 1 | X_i) = \operatorname{logit}^{-1}(X_i \beta)
\]</span></p>
<p>donde <span class="math inline">\(y_i\)</span> es la respuesta binaria, <span class="math inline">\(\operatorname{logit}^{- 1}\)</span> es la función logit inversa y <span class="math inline">\(X_i \beta\)</span> es el predictor lineal. Podemos interpretar esta formulación diciendo que la probabilidad de que <span class="math inline">\(y = 1\)</span> es igual al logit inverso del predictor lineal <span class="math inline">\((X_i, \ beta)\)</span>. Por lo tanto, podemos expresar los valores ajustados del modelo logístico y los coeficientes como probabilidades utilizando la transformación logit inversa. Pero, ¿qué es exactamente el logit inverso? Pues es:</p>
<p><span class="math display">\[\operatorname{logit}^{-1}(x) = \frac{e^{x}}{1 + e^{x}}\]</span></p>
<p>donde <span class="math inline">\(e\)</span> es la función exponencial.</p>
<p>Podemos tener una idea de cómo la función logit inversa transforma el predictor lineal mediante una gráfica. Aquí usamos un rango arbitrario de valores de x en (-6, 6) para demostrar la transformación.</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="regresión-logística.html#cb305-1"></a>x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="fl">.01</span>)</span>
<span id="cb305-2"><a href="regresión-logística.html#cb305-2"></a>y &lt;-<span class="st"> </span><span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x))</span>
<span id="cb305-3"><a href="regresión-logística.html#cb305-3"></a><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y), <span class="kw">aes</span>(x, y)) <span class="op">+</span></span>
<span id="cb305-4"><a href="regresión-logística.html#cb305-4"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb305-5"><a href="regresión-logística.html#cb305-5"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(logit<span class="op">^-</span><span class="dv">1</span>,<span class="st">&quot;(x)&quot;</span>))) <span class="op">+</span></span>
<span id="cb305-6"><a href="regresión-logística.html#cb305-6"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;y = &quot;</span>, logit<span class="op">^-</span><span class="dv">1</span>,<span class="st">&quot;(x)&quot;</span>)))</span></code></pre></div>
<p><img src="fig/unnamed-chunk-92-1.png" width="672" /></p>
<p>Los valores <span class="math inline">\(x\)</span>, que van de -6 a 6, son comprimidos por la función logit inversa en el rango 0-1. El logit inverso es curvo, por lo que la diferencia esperada en <span class="math inline">\(y\)</span> correspondiente a una diferencia fija en <span class="math inline">\(x\)</span> no es constante. A valores bajos y valores altos de <span class="math inline">\(x\)</span>, un cambio de unidad corresponde a un cambio muy pequeño en <span class="math inline">\(y\)</span>, mientras que en la mitad de la curva un pequeño cambio en <span class="math inline">\(x\)</span> corresponde a un cambio relativamente grande en <span class="math inline">\(y\)</span>. En la regresión lineal, la diferencia esperada en <span class="math inline">\(y\)</span> correspondiente a una diferencia fija en <span class="math inline">\(x\)</span> es, por el contrario, constante. <em>Por lo tanto, cuando interpretamos los resultados logísticos debemos elegir en qué parte de la curva queremos evaluar la probabilidad del resultado, dado el modelo.</em></p>
</div>
<div id="ejemplo-de-regresión-logística" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Ejemplo de regresión logística</h2>
<p>Ilustremos estos conceptos utilizando el conjunto de datos “Default” del ISLR. Este conjunto de datos simulado contiene una variable binaria que representa el incumplimiento en los pagos de la tarjeta de crédito (variable “default”), que modelaremos como una función de la variable “balance” (la cantidad de deuda que tiene la tarjeta) y la variable “income” (ingresos). Primero visualizaremos cómo es esta relación.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="regresión-logística.html#cb306-1"></a><span class="kw">library</span>(ISLR)</span>
<span id="cb306-2"><a href="regresión-logística.html#cb306-2"></a><span class="kw">data</span>(Default)</span>
<span id="cb306-3"><a href="regresión-logística.html#cb306-3"></a><span class="kw">str</span>(Default)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:	10000 obs. of  4 variables:
##  $ default: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ student: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 2 1 1 ...
##  $ balance: num  730 817 1074 529 786 ...
##  $ income : num  44362 12106 31767 35704 38463 ...</code></pre>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="regresión-logística.html#cb308-1"></a><span class="kw">ggplot</span>(Default, <span class="kw">aes</span>(<span class="dt">x =</span> balance, <span class="dt">y =</span> income, <span class="dt">col =</span> default)) <span class="op">+</span></span>
<span id="cb308-2"><a href="regresión-logística.html#cb308-2"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">.4</span>) <span class="op">+</span></span>
<span id="cb308-3"><a href="regresión-logística.html#cb308-3"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Balance vs. Income by Default&quot;</span>)</span></code></pre></div>
<p><img src="fig/unnamed-chunk-93-1.png" width="672" /></p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="regresión-logística.html#cb309-1"></a><span class="kw">grid.arrange</span>(</span>
<span id="cb309-2"><a href="regresión-logística.html#cb309-2"></a><span class="kw">ggplot</span>(Default, <span class="kw">aes</span>(default, balance)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb309-3"><a href="regresión-logística.html#cb309-3"></a><span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span></span>
<span id="cb309-4"><a href="regresión-logística.html#cb309-4"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Balance by Default&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb309-5"><a href="regresión-logística.html#cb309-5"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;balance&quot;</span>),</span>
<span id="cb309-6"><a href="regresión-logística.html#cb309-6"></a><span class="kw">ggplot</span>(Default, <span class="kw">aes</span>(default, income)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb309-7"><a href="regresión-logística.html#cb309-7"></a><span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span></span>
<span id="cb309-8"><a href="regresión-logística.html#cb309-8"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Income by Default&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb309-9"><a href="regresión-logística.html#cb309-9"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;income&quot;</span>),</span>
<span id="cb309-10"><a href="regresión-logística.html#cb309-10"></a><span class="dt">ncol =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="fig/unnamed-chunk-93-2.png" width="672" /></p>
<p>Claramente, los valores altos de saldo están asociados con el incumplimiento en todos los niveles de ingresos. Esto sugiere que los ingresos en realidad no son un fuerte predictor de incumplimiento, en comparación con el saldo, que es exactamente lo que vemos en los diagramas de cajas.</p>
<p>Exploremos estas relaciones usando la regresión logística. En R ajustamos un modelo logístico usando la función <code>glm ()</code> con <code>family = binomial</code>.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> Centraremos y escalaremos las variables para facilitar la interpretación.</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="regresión-logística.html#cb310-1"></a><span class="kw">glm</span>(default <span class="op">~</span><span class="st"> </span>balance <span class="op">+</span><span class="st"> </span>income, </span>
<span id="cb310-2"><a href="regresión-logística.html#cb310-2"></a>      <span class="dt">data =</span> Default, </span>
<span id="cb310-3"><a href="regresión-logística.html#cb310-3"></a>      <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span></span>
<span id="cb310-4"><a href="regresión-logística.html#cb310-4"></a><span class="st">    </span>standardize <span class="op">%&gt;%</span></span>
<span id="cb310-5"><a href="regresión-logística.html#cb310-5"></a><span class="st">    </span>display</span></code></pre></div>
<pre><code>## glm(formula = default ~ z.balance + z.income, family = binomial, 
##     data = Default)
##             coef.est coef.se
## (Intercept) -6.13     0.19  
## z.balance    5.46     0.22  
## z.income     0.56     0.13  
## ---
##   n = 10000, k = 3
##   residual deviance = 1579.0, null deviance = 2920.6 (difference = 1341.7)</code></pre>
<p>NOTA: Se puede apreciar la ventaja del uso de tidyverse (pipe) - no se necesita crear las variables estandarizadas, ni guardar el resultado para luego hacer un print</p>
<p>Interpretamos esta salida exactamente como lo haríamos para un modelo lineal con un predictor centrado y escalado:</p>
<ul>
<li><em>Intercept</em>: -6,13 representa las probabilidades logarítmicas (log odds) de incumplimiento cuando el saldo es promedio (835.37) y el ingreso es promedio (3.351698^{4} ). (Promedio porque las variables se han centrado).</li>
<li><em>z.balance</em>: 5.46 representa el cambio predicho en las probabilidades logarítmicas de incumplimiento asociado con un aumento de 1 unidad en el saldo (z.balance), manteniendo constante el ingreso (z.income). Un aumento de 1 unidad en el saldo (z.balance) es equivalente a un aumento de 2 desviaciones estándar en el saldo (balance) (967.43). Este coeficiente es estadísticamente significativo ya que 5.46 - 2 x .22 &gt; 0 (el IC del 95% que no contiene 0 indica significación estadística).</li>
<li><em>z.income</em>: .56 representa el cambio predicho en las probabilidades logarítmicas (log odds) de incumplimiento asociadas con un aumento de 1 unidad en el ingreso (z.income), manteniendo constante el balance (z.balance). Un aumento de 1 unidad en el ingreso (z.income) es equivalente a un aumento de 2 desviaciones estándar en el ingreso (income) (2.667328^{4}). Este coeficiente también es estadísticamente significativo ya que .56 - 2 x .13 &gt; 0.</li>
</ul>
<p>¿Qué significa que las probabilidades logarítmicas de incumplimiento aumenten en 5.46 o .56? En términos precisos, ¿quién sabe? Para que estas cantidades tengan una mejor interpretación, necesitamos transformarlas, ya sea en probabilidades (odds) o en razones de probabilidades (razón de odds -&gt; odds ratio). Sin embargo, debemos señalar que el signo y la magnitud de los coeficientes si son informativas: la relación con el incumplimiento del pago es positiva en ambos casos y, como ya se había visto de forma gráfica en los diagramas de cajas, el efecto del saldo (balance) es mucho mayor que el del ingreso (income).</p>
</div>
<div id="coeficientes-de-regresión-logística-como-probabilidades" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Coeficientes de regresión logística como probabilidades</h2>
<p>Podemos dar una interpretación más específica de la regresión logística más allá del efecto y magnitud. Para ello, podemos usar la función logit inversa para convertir las probabilidades logarítmicas (log-odds) de incumplimiento de pago en las tarjetas (cuando el saldo y los ingresos son promedio) en una probabilidad:</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="regresión-logística.html#cb312-1"></a>invlogit &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x))</span>
<span id="cb312-2"><a href="regresión-logística.html#cb312-2"></a> <span class="kw">invlogit</span>(<span class="op">-</span><span class="fl">6.13</span> <span class="op">+</span><span class="st"> </span><span class="fl">5.46</span> <span class="op">*</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span><span class="fl">.56</span> <span class="op">*</span><span class="st"> </span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] 0.002171854</code></pre>
<p>La probabilidad de incumplimiento para aquellos con un saldo promedio de tarjeta de crédito de (835.37) y un ingreso promedio de (3.351698^{4}) es de hecho bastante bajo: solo 0.002. Asimismo, podemos calcular el cambio en la probabilidad de incumplimiento en el pago asociado con un aumento de 1 unidad en el saldo, manteniendo el ingreso constante en el promedio (z.ingreso=0). Esto equivaldría a aumentar el saldo en casi 1000$, de 835.37 a 1802.8.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="regresión-logística.html#cb314-1"></a><span class="kw">invlogit</span>(<span class="op">-</span><span class="fl">6.13</span> <span class="op">+</span><span class="st"> </span><span class="fl">5.46</span> <span class="op">*</span><span class="st"> </span><span class="dv">1</span>) <span class="op">-</span><span class="st"> </span><span class="kw">invlogit</span>(<span class="op">-</span><span class="fl">6.13</span> <span class="op">+</span><span class="st"> </span><span class="fl">5.46</span> <span class="op">*</span><span class="st"> </span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] 0.336325</code></pre>
</div>
<div id="coeficientes-de-regresión-logística-como-razones-de-odds" class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> Coeficientes de regresión logística como razones de odds</h2>
<p>También podemos interpretar los coeficientes de regresión logística como <em>razones de odds</em> (OR).<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> Si dos resultados tienen probabilidades <span class="math inline">\((p, 1-p)\)</span>, entonces <span class="math inline">\(\frac {p} {1-p}\)</span> se conoce como <em>odds</em> (probabilidades o momio) del resultado. Las <em>odds</em> son simplemente diferentes formas de representar la misma información: <span class="math inline">\(\text{odds} = \frac{p}{1-p}\)</span> y <span class="math inline">\(p = \frac{\text{odds}} {1+ \text{odds}}\)</span>. Por ejemplo, una <em>odds</em> de 1 es equivalente a una <em>odds</em> de .5 — es decir, resultados igualmente probables para <span class="math inline">\(p\)</span> y <span class="math inline">\(1-p\)</span>: <span class="math inline">\(\text{odds(p = .5)} = \frac{.5}{1-.5} = 1\)</span> y <span class="math inline">\(p(\text{oods} = 1) = \frac{\text{1}}{1 + 1} = .5.\)</span></p>
<p>La razón de dos odds es una OR:</p>
<p><span class="math display">\[
\frac{\frac{p_2}{1-p_2}}{\frac{p_1}{1-p_1}}
\]</span></p>
<p>Una razón de <em>odds</em> se puede interpretar como un cambio en la probabilidad. Por ejemplo, un cambio en la probabilidad de <span class="math inline">\(p_1 = .33\)</span> a <span class="math inline">\(p_2 = .5\)</span> da como resultado un OR de 2, de la siguiente manera:</p>
<p><span class="math display">\[
\frac{\frac{.5}{.5}}{\frac{.33}{.66}} = \frac{1}{.5} = 2
\]</span></p>
<p>También podemos interpretar el OR como el aumento porcentual de las probabilidades de un evento. Aquí, un OR de 2 equivale a aumentar las probabilidades en un 100%, de 0,5 a 1.</p>
<p>Recordemos que representamos el modelo logit de esta manera:</p>
<p><span class="math display">\[
\text{log} \left(\frac{p}{1-p}\right) = \alpha + \beta x
\]</span></p>
<p>La parte izquierda de la ecuación, expresado como logaritmos de probabilidades, está en la misma escala que la derecha derecho: <span class="math inline">\(\pm \infty\)</span>. Por lo tanto, no hay no linealidad en esta relación, y aumentar <span class="math inline">\(x\)</span> en 1 unidad tiene el mismo efecto que en la regresión lineal: cambia el resultado predicho en <span class="math inline">\(\beta\)</span>. Entonces, pasar de <span class="math inline">\(x\)</span> a <span class="math inline">\(x + 1\)</span> equivale a sumar <span class="math inline">\(\beta\)</span> a ambos lados de la ecuación anterior. Centrándonos solo en el lado izquierdo, tenemos, después de exponenciar, las probabilidades originales multiplicadas por <span class="math inline">\(e^\beta\)</span>:</p>
<p><span class="math display">\[
e^{\text{log} \left(\frac{p}{1-p}\right) + \beta} = \frac{p}{1-p} *e^ \beta
\]</span></p>
<p>(ya que <span class="math inline">\(e^{a+b} = e^a*e^b\)</span>).</p>
<p>Podemos pensar en <span class="math inline">\(e^\beta\)</span> como el cambio de la odds del resultado cuando <span class="math inline">\(x\)</span> cambia en 1 unidad, que se puede representar, utilizando la formulación anterior, como una OR:</p>
<p><span class="math display">\[
\frac{\frac{p_2}{1-p_2}}{\frac{p_1}{1-p_1}} = \frac{\frac{p_1}{1-p_1} * e^\beta 
}{\frac{p_1}{1-p_1}} = e^\beta.
\]</span></p>
<p>Por lo tanto, <span class="math inline">\(e^\beta\)</span> se puede interpretar como el cambio en las probabilidades asociadas con un aumento de 1 unidad en <span class="math inline">\(x\)</span>, expresado en términos porcentuales. En el caso de OR = <span class="math inline">\(\frac{1}{. 5} = 2\)</span>, el porcentaje de aumento en las probabilidades del resultado es del 100%. Cuando la OR es <span class="math inline">\(&gt;2\)</span> se suele expresar como x-veces más (OR=3.5, hay 3.5 veces más probabilidad de observar el evento que no observarlo cuando se cambia <span class="math inline">\(x\)</span> en 1 unidad), y cuando la OR es <span class="math inline">\(&lt;1\)</span> se suele hablar de protección a no tener el evento y el porcentaje se calcula como 1-OR.</p>
<p>Apliquemos esta información a nuestro modelo anterior de aplicando la exponencial a los coeficientes de saldo e ingresos:</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="regresión-logística.html#cb316-1"></a><span class="kw">exp</span>(<span class="fl">5.46</span>)</span></code></pre></div>
<pre><code>## [1] 235.0974</code></pre>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="regresión-logística.html#cb318-1"></a><span class="kw">exp</span>(.<span class="dv">56</span>)</span></code></pre></div>
<pre><code>## [1] 1.750673</code></pre>
<p>Podemos interpretar estos ORs como el porcentaje o cambio multiplicativo en las probabilidades asociadas con un aumento de 1 unidad en el predictor (mientras se mantienen constantes los demás), de 1 a 235 (un aumento de 23,400%) en el caso de balance, y de 1 a 1,75 (un aumento del 75%) para los ingresos. Por ejemplo, podemos decir que la probabilidad de incumplimiento es un 75% mayor cuando los ingresos aumentan en 1 unidad.</p>
<p>Cuando las variables predictoras son categóricas (como en biomedicina: sexo, estadío tumoral, fumar, beber, …) la interpretación se hace más sencilla porque el cambio de 1 unidad en estas variables, supone el cambio de una categoría respecto a la basal (ya que se usan <em>dummy variables</em>). Así, por ejemplo, si nuestro outcomes tener cáncer de pulmón o no, y nuestro predictor es ser fumanor o no, si nuestros análisis nos dan una OR de 6 asociado a ser fumador, la interpretación sería: "La odds (probabilidad, abusando de lenguaje - también riesgo si el outcome es poco frecuente) de sufrir cáncer de pulmón es 6 veces mayor en los fumadores que en los no fumadores.</p>
</div>
<div id="bondad-de-ajuste" class="section level2" number="6.5">
<h2><span class="header-section-number">6.5</span> Bondad de ajuste</h2>
<p>Podemos evaluar el rendimiento del modelo logístico utilizando el AIC, así como mediante el uso de otras medidas como: la desviación (<em>deviance</em>) residual, la precisión, la sensibilidad, la especificidad y el área bajo la curva (AUC).</p>
<p>Al igual que AIC, la <em>deviance</em> es una medida de error, por lo que una <em>deviance</em> más baja significa un mejor ajuste a los datos. Esperamos que la desviación disminuya en 1 para cada predictor, por lo que con un predictor informativo (e.g. variable imporante para el modleo), la <em>deviance</em> disminuirá en más de 1. Deviance = <span class="math inline">\(-2ln(L)\)</span>, donde <span class="math inline">\(ln\)</span> es el logaritmo natural y <span class="math inline">\(L\)</span> es la función de verosimilitud . Veámoslo con nuestro ejemplo:</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="regresión-logística.html#cb320-1"></a>logistic_model1 &lt;-<span class="st"> </span></span>
<span id="cb320-2"><a href="regresión-logística.html#cb320-2"></a><span class="st">  </span><span class="kw">glm</span>(default <span class="op">~</span><span class="st"> </span>balance, </span>
<span id="cb320-3"><a href="regresión-logística.html#cb320-3"></a>      <span class="dt">data =</span> Default, </span>
<span id="cb320-4"><a href="regresión-logística.html#cb320-4"></a>      <span class="dt">family =</span> binomial)</span>
<span id="cb320-5"><a href="regresión-logística.html#cb320-5"></a>logistic_model1<span class="op">$</span>deviance</span></code></pre></div>
<pre><code>## [1] 1596.452</code></pre>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="regresión-logística.html#cb322-1"></a>logistic_model2 &lt;-<span class="st"> </span></span>
<span id="cb322-2"><a href="regresión-logística.html#cb322-2"></a><span class="st">  </span><span class="kw">glm</span>(default <span class="op">~</span><span class="st"> </span>balance <span class="op">+</span><span class="st"> </span>income, </span>
<span id="cb322-3"><a href="regresión-logística.html#cb322-3"></a>      <span class="dt">data =</span> Default, </span>
<span id="cb322-4"><a href="regresión-logística.html#cb322-4"></a>      <span class="dt">family =</span> binomial)</span>
<span id="cb322-5"><a href="regresión-logística.html#cb322-5"></a>logistic_model2<span class="op">$</span>deviance</span></code></pre></div>
<pre><code>## [1] 1578.966</code></pre>
<p>En este caso, la <em>deviance</em> se redujo en más de 1, lo que indica que los ingresos mejoran el ajuste del modelo. Podemos hacer una prueba formal de la diferencia usando, como para los modelos lineales, la prueba de razón de verosimilitud:</p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="regresión-logística.html#cb324-1"></a><span class="kw">lrtest</span>(logistic_model1, logistic_model2)</span></code></pre></div>
<pre><code>## 
## Model 1: default ~ balance
## Model 2: default ~ balance + income
## 
##   L.R. Chisq         d.f.            P 
## 1.748541e+01 1.000000e+00 2.895205e-05</code></pre>
<p>También podemos traducir las probabilidades de un modelo logístico para el incumplimiento del pago en predicciones de clase asignando “Sí” (predeterminado) a probabilidades mayores o iguales a .5 y “No” (sin valor predeterminado) a probabilidades menores que .5, y luego contar el número de veces que el modelo asigna la clase predeterminada correcta. Si dividimos este número por el total de observaciones, habremos calculado la “precisión”. La precisión se utiliza a menudo como medida del rendimiento del modelo.</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="regresión-logística.html#cb326-1"></a>preds &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">fitted</span>(logistic_model2) <span class="op">&gt;=</span><span class="st"> </span><span class="fl">.5</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>) </span>
<span id="cb326-2"><a href="regresión-logística.html#cb326-2"></a>(<span class="kw">length</span>(<span class="kw">which</span>(preds <span class="op">==</span><span class="st"> </span>Default<span class="op">$</span>default)) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(Default))<span class="op">*</span><span class="dv">100</span></span></code></pre></div>
<pre><code>## [1] 97.37</code></pre>
<p>El modelo es 97.37% preciso. Valores superirores al 50% mostrarían una mejora en la predicción ya que por azar, se espera que el modelo tenga una precisión del 50%.</p>
<table>
<tbody>
<tr class="odd">
<td align="left"><strong>EJERCICIO</strong> (Entrega Moodle P2-Precisión)</td>
</tr>
<tr class="even">
<td align="left">Utiliza una simulación sencilla para demostrar que is asignamos por azar que una persona va a incumplir o no con los pagos, el valor esperado de la precisión de la variable “Default$default” es del 50%. NOTA: realiza 1000 simulaciones.</td>
</tr>
</tbody>
</table>
<p>Una forma sencilla de obtener un buen modelo de clasificación sería asignar a todos la categoría más frecuente. Por ejemplo, en nuestros datos, la clase mayoritaria es “No” para la variable incimpliminto por un amplio margen (9667 a 333). La mayoría de las personas no incumplen. ¿Cuál es nuestra precisión, entonces, si simplemente predecimos “No” para todos los casos? La proporción de “No” en los datos es 96.67%, por lo que si siempre predijimos “No” esa sería nuestra precisión (9667 / (9667 + 333) = .9667). El modelo logístico, sorprendentemente, ofrece solo una ligera mejora.</p>
<p>Sin embargo, al evaluar el rendimiento del clasificador, debemos reconocer que no todos los errores son iguales y que la precisión tiene limitaciones como métrica de rendimiento. En algunos casos, el modelo puede haber predicho el incumplimiento cuando no lo había. Esto se conoce como “falso positivo”. En otros casos, el modelo puede haber predicho que no hubo incumplimiento cuando hubo incumplimiento. Esto se conoce como “falso negativo”. En la clasificación, utilizamos lo que se conoce como matriz de confusión para resumir estos diferentes tipos de errores del modelo, denominados así porque la matriz resume cómo se confunde el modelo. Usaremos la función <code>confusionMatrix ()</code> de la librería <code>caret</code> para calcular rápidamente estos valores:</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="regresión-logística.html#cb328-1"></a><span class="kw">confusionMatrix</span>(<span class="kw">as.factor</span>(preds), Default<span class="op">$</span>default, <span class="dt">positive =</span> <span class="st">&quot;Yes&quot;</span>)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  9629  225
##        Yes   38  108
##                                           
##                Accuracy : 0.9737          
##                  95% CI : (0.9704, 0.9767)
##     No Information Rate : 0.9667          
##     P-Value [Acc &gt; NIR] : 3.067e-05       
##                                           
##                   Kappa : 0.4396          
##                                           
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
##                                           
##             Sensitivity : 0.3243          
##             Specificity : 0.9961          
##          Pos Pred Value : 0.7397          
##          Neg Pred Value : 0.9772          
##              Prevalence : 0.0333          
##          Detection Rate : 0.0108          
##    Detection Prevalence : 0.0146          
##       Balanced Accuracy : 0.6602          
##                                           
##        &#39;Positive&#39; Class : Yes             
## </code></pre>
<p>Esta función produce una gran cantidad de resultados. Podemos ver que informa la misma precisión que calculamos anteriormente: .97. La matriz de confusión 2 x 2 está en la parte superior. Podemos caracterizar estos 4 valores en la matriz de la siguiente manera:</p>
<ol style="list-style-type: decimal">
<li>9629 negativos verdaderos (TN): cuando el modelo predice correctamente “No”</li>
<li>108 verdaderos positivos (TP): cuando el modelo predice correctamente “Sí”</li>
<li>225 falsos negativos (FN): cuando el modelo predice incorrectamente “No”</li>
<li>38 falsos positivos (FP): cuando el modelo predice incorrectamente “Sí”</li>
</ol>
<p>La siguiente tabla resume estas posibilidades:</p>
<table>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="left">Reference</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Predicted</td>
<td align="left">No</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">No</td>
<td align="left">TN</td>
<td align="left">FN</td>
</tr>
<tr class="even">
<td align="left">Yes</td>
<td align="left">FP</td>
<td align="left">TP</td>
</tr>
</tbody>
</table>
<p>Hay dos medidas clave, además de la precisión, para caracterizar el rendimiento del modelo. Mientras que la precisión mide el error general, la sensibilidad y la especificidad miden errores específicos de clase.</p>
<ul>
<li><em>Precisión</em> = 1 - (FP + FN) / Total:</li>
</ul>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="regresión-logística.html#cb330-1"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">38</span> <span class="op">+</span><span class="st"> </span><span class="dv">225</span>) <span class="op">/</span><span class="st"> </span><span class="dv">10000</span></span></code></pre></div>
<pre><code>## [1] 0.9737</code></pre>
<ul>
<li><em>Sensibilidad</em> (o la tasa de verdaderos positivos): TP / (TP + FN). En este caso, la sensibilidad mide la proporción de incumplimientos que se clasificaron correctamente como tales.</li>
</ul>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="regresión-logística.html#cb332-1"></a><span class="dv">108</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">108</span> <span class="op">+</span><span class="st"> </span><span class="dv">225</span>) </span></code></pre></div>
<pre><code>## [1] 0.3243243</code></pre>
<ul>
<li><em>Especificidad</em> (o la tasa de verdaderos negativos): TN / (TN + FP). En este caso, la especificidad mide la proporción de no incumplimientos que se clasificaron correctamente como tales.</li>
</ul>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="regresión-logística.html#cb334-1"></a><span class="dv">9629</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">9629</span> <span class="op">+</span><span class="st"> </span><span class="dv">38</span>) </span></code></pre></div>
<pre><code>## [1] 0.9960691</code></pre>
<p>¿Por qué deberíamos considerar estos errores específicos de clase? Todos los modelos tienen errores, pero no todos los errores del modelo son igualmente importantes. Por ejemplo, un falso negativo — prediciendo incorrectamente que un prestatario no incurrirá en incumplimiento — puede ser un error costoso para un banco, si el incumplimiento se hubiera podido prevenir mediante la intervención. Pero, por otro lado, un falso positivo, que predice incorrectamente que un prestatario incurrirá en incumplimiento, puede desencadenar una advertencia innecesaria que irrita a los clientes. Los errores que comete un modelo se pueden controlar ajustando el umbral de decisión utilizado para asignar probabilidades predichas a las clases. Usamos un umbral de probabilidad de .5 para clasificar los incumplimientos en los pagos. Si el umbral se establece en .1, por el contrario, la precisión general disminuiría, pero también lo haría el número de falsos negativos, lo que podría ser deseable. El modelo luego atraparía a más morosos, lo que ahorraría dinero al banco, pero eso tendría un costo: más falsos positivos (clientes potencialmente irritados).</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="regresión-logística.html#cb336-1"></a>preds &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">ifelse</span>(<span class="kw">fitted</span>(logistic_model2) <span class="op">&gt;=</span><span class="st"> </span><span class="fl">.1</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>))</span>
<span id="cb336-2"><a href="regresión-logística.html#cb336-2"></a><span class="kw">confusionMatrix</span>(preds, Default<span class="op">$</span>default)<span class="op">$</span>table</span></code></pre></div>
<pre><code>##           Reference
## Prediction   No  Yes
##        No  9105   90
##        Yes  562  243</code></pre>
<p>La pregunta de cómo establecer el umbral de decisión —. 5, .1 o algo más — debe responderse con referencia al contexto empresarial.</p>
<p>Una curva de característica operativa del receptor (ROC por sus siglas en ingles) visualiza las compensaciones entre los tipos de errores trazando la especificidad frente a la sensibilidad. El cálculo del área bajo la curva ROC (AUC) nos permite, además, resumir el rendimiento del modelo y comparar modelos. La curva en sí muestra los tipos de errores que cometería el modelo en diferentes umbrales de decisión. Para crear una curva ROC usamos la función <code>roc ()</code> del paquete pROC, y mostramos los valores de sensibilidad / especificidad asociados con los umbrales de decisión de .1 y .5:</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="regresión-logística.html#cb338-1"></a><span class="kw">library</span>(pROC)</span>
<span id="cb338-2"><a href="regresión-logística.html#cb338-2"></a><span class="kw">library</span>(plotROC)</span>
<span id="cb338-3"><a href="regresión-logística.html#cb338-3"></a><span class="kw">invisible</span>(<span class="kw">plot</span>(<span class="kw">roc</span>(<span class="kw">factor</span>(<span class="kw">ifelse</span>(Default<span class="op">$</span>default <span class="op">==</span><span class="st"> &quot;Yes&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)), </span>
<span id="cb338-4"><a href="regresión-logística.html#cb338-4"></a>                   <span class="kw">fitted</span>(logistic_model2)), <span class="dt">print.thres =</span> <span class="kw">c</span>(.<span class="dv">1</span>, <span class="fl">.5</span>), </span>
<span id="cb338-5"><a href="regresión-logística.html#cb338-5"></a>               <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">print.auc =</span> T))</span></code></pre></div>
<p><img src="fig/unnamed-chunk-107-1.png" width="672" /></p>
<p>Un modelo con una precisión del 50%, es decir, un clasificador aleatorio, tendría una curva ROC que siguiera la línea de referencia diagonal. Un modelo con una precisión del 100%, un clasificador perfecto, tendría una curva ROC siguiendo los márgenes del triángulo superior. Cada punto de la curva ROC representa un par de sensibilidad / especificidad correspondiente a un umbral de decisión particular. Cuando establecimos el umbral de decisión en .1, la sensibilidad fue .73 (243 / (243 + 90)) y la especificidad fue .94 (9105 / (9105 + 562)). Ese punto se muestra en la curva. Del mismo modo, cuando establecimos el umbral de decisión en .5, la sensibilidad fue .32 y la especificidad fue .996. Ese punto también está en la curva. ¿Qué umbral de decisión es óptimo? Nuevamente, depende del problema (pensad en cáncer o en este ejemplo de dinero). Las curvas ROC nos permiten elegir los errores específicos de clase que podemos cometer.</p>
<p>El AUC es el resumen de cómo funciona el modelo en cada umbral de decisión. En general, los modelos con AUC más altos son mejores. Esta medida nos servirá para comparar métodos de aprendizaje automático que iremos aprendiendo durante el curso.</p>
</div>
<div id="ejemplo-de-regresión-logística-modelización-de-riesgo-diabetes" class="section level2" number="6.6">
<h2><span class="header-section-number">6.6</span> Ejemplo de regresión logística: modelización de riesgo diabetes</h2>
<p>Para este ejemplo usaremos el conjunto de datos Pima, incluido en la librería <code>MASS</code> que contienen esta información:</p>
<blockquote>
<p>Una población de mujeres que tenían al menos 21 años, de ascendencia indígena Pima y que vivían cerca de Phoenix, Arizona, se sometieron a pruebas de diabetes de acuerdo con los criterios de la Organización Mundial de la Salud. Los datos fueron recopilados por el Instituto Nacional de Diabetes y Enfermedades Digestivas y Renales de EE. UU. Usaremos información para 532 mujeres con datos completos después de eliminar los datos (principalmente faltantes) sobre la insulina sérica.</p>
</blockquote>
<p>El conjunto de datos incluye las siguientes variables:</p>
<ol style="list-style-type: decimal">
<li>npreg: número de embarazos</li>
<li>glu: concentración de glucosa en plasma a 2 horas en una prueba de tolerancia oral a la glucosa</li>
<li>bp: presión arterial diastólica (mm Hg)</li>
<li>piel: grosor del pliegue cutáneo del tríceps (mm)</li>
<li>bmi: índice de masa corporal (peso en kg / (altura en m) ^ 2)</li>
<li>ped: función del pedigrí de la diabetes</li>
<li>age: Edad (años)</li>
<li>type: Sí o No (diabetes)</li>
</ol>
<p>La variable resultado es “type”, que indica si una persona tiene diabetes. Dividiremos los datos en un conjunto de entrenamiento y otro test que combinaremos para ilustrar este ejemplo.</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="regresión-logística.html#cb339-1"></a><span class="kw">library</span>(MASS)</span>
<span id="cb339-2"><a href="regresión-logística.html#cb339-2"></a><span class="kw">data</span>(<span class="st">&quot;Pima.tr&quot;</span>)</span>
<span id="cb339-3"><a href="regresión-logística.html#cb339-3"></a><span class="kw">data</span>(<span class="st">&quot;Pima.te&quot;</span>)</span>
<span id="cb339-4"><a href="regresión-logística.html#cb339-4"></a>d &lt;-<span class="st"> </span><span class="kw">rbind</span>(Pima.te, Pima.tr)</span>
<span id="cb339-5"><a href="regresión-logística.html#cb339-5"></a><span class="kw">str</span>(d)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:	532 obs. of  8 variables:
##  $ npreg: int  6 1 1 3 2 5 0 1 3 9 ...
##  $ glu  : int  148 85 89 78 197 166 118 103 126 119 ...
##  $ bp   : int  72 66 66 50 70 72 84 30 88 80 ...
##  $ skin : int  35 29 23 32 45 19 47 38 41 35 ...
##  $ bmi  : num  33.6 26.6 28.1 31 30.5 25.8 45.8 43.3 39.3 29 ...
##  $ ped  : num  0.627 0.351 0.167 0.248 0.158 0.587 0.551 0.183 0.704 0.263 ...
##  $ age  : int  50 31 21 26 53 51 31 33 27 29 ...
##  $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 2 2 2 2 1 1 2 ...</code></pre>
<p>Todos los predictores son enteros o numéricos. Nuestro objetivo es construir un modelo logístico de diabetes para ilustrar cómo interpretar los coeficientes del modelo.</p>
<div id="modelo-simple" class="section level3" number="6.6.1">
<h3><span class="header-section-number">6.6.1</span> Modelo simple</h3>
<p>Comencemos con un modelo simple.</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="regresión-logística.html#cb341-1"></a>bin_model1 &lt;-<span class="st"> </span></span>
<span id="cb341-2"><a href="regresión-logística.html#cb341-2"></a><span class="st">  </span><span class="kw">glm</span>(type <span class="op">~</span><span class="st"> </span>bmi <span class="op">+</span><span class="st"> </span>age, </span>
<span id="cb341-3"><a href="regresión-logística.html#cb341-3"></a>      <span class="dt">data =</span> d, </span>
<span id="cb341-4"><a href="regresión-logística.html#cb341-4"></a>      <span class="dt">family =</span> binomial)</span>
<span id="cb341-5"><a href="regresión-logística.html#cb341-5"></a><span class="kw">display</span>(bin_model1)</span></code></pre></div>
<pre><code>## glm(formula = type ~ bmi + age, family = binomial, data = d)
##             coef.est coef.se
## (Intercept) -6.26     0.67  
## bmi          0.10     0.02  
## age          0.06     0.01  
## ---
##   n = 532, k = 3
##   residual deviance = 577.2, null deviance = 676.8 (difference = 99.6)</code></pre>
<ul>
<li><em>Intercept</em>: -6.26 es el logaritmo de la probabilidad de tener diabetes cuando bmi = 0 y edad = 0. Dado que ni la edad ni el bmi pueden ser iguales a 0, el intercept no es interpretable en este modelo. Por tanto, tendría sentido centrar las variables para facilitar la interpretación.</li>
<li><em>bmi</em>: .1 es el cambio previsto en el <em>log-odds</em> de diabetes asociado con un aumento de 1 unidad en el bmi, manteniendo la edad constante. Este coeficiente es estadísticamente significativo ya que .1 - 2 x .02 &gt; 0. (Un IC del 95% que no contiene 0 indica significancia estadística) y también porque su p-valor asociacio mediante el test de score es <span class="math inline">\(&lt;0.05\)</span> (usar la función <code>summary ()</code> . Podemos traducir este coeficiente en un OR mediante la exponencial: <span class="math inline">\(e^.1\)</span> = 1.11. Un aumento de 1 unidad en el IMC, manteniendo la edad constante, se asocia con un aumento del 11% en la <em>odds</em> (o, más coloquialmente, la probabilidad) de diabetes.</li>
<li><em>edad</em>: .06 es el cambio predicho en el <em>log-oods</em> de diabetes asociado con un aumento de 1 unidad en la edad, manteniendo constante el bmi. Este coeficiente también es estadísticamente significativo ya que .06 - 2 x .01&gt; 0. El OR para la edad es <span class="math inline">\(e^.06\)</span> = 1.06 lo que indica un aumento del 6% en la probabilidad de sufrir diabetes asociada con un aumento de 1 unidad en la edad.</li>
</ul>
</div>
<div id="agregar-predictores-y-evaluar-el-ajuste" class="section level3" number="6.6.2">
<h3><span class="header-section-number">6.6.2</span> Agregar predictores y evaluar el ajuste</h3>
<p>Ahora ajustaremos un modelo completo (excluyendo “skin”, ya que parece medir casi lo mismo que bmi). ¿Mejora el ajuste?</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="regresión-logística.html#cb343-1"></a>bin_model2 &lt;-<span class="st"> </span></span>
<span id="cb343-2"><a href="regresión-logística.html#cb343-2"></a><span class="st">  </span><span class="kw">glm</span>(type <span class="op">~</span><span class="st"> </span>bmi <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>ped <span class="op">+</span><span class="st"> </span>glu <span class="op">+</span><span class="st"> </span>npreg <span class="op">+</span><span class="st"> </span>bp , </span>
<span id="cb343-3"><a href="regresión-logística.html#cb343-3"></a>      <span class="dt">data =</span> d, </span>
<span id="cb343-4"><a href="regresión-logística.html#cb343-4"></a>      <span class="dt">family =</span> binomial) </span>
<span id="cb343-5"><a href="regresión-logística.html#cb343-5"></a><span class="kw">display</span>(bin_model2)</span></code></pre></div>
<pre><code>## glm(formula = type ~ bmi + age + ped + glu + npreg + bp, family = binomial, 
##     data = d)
##             coef.est coef.se
## (Intercept) -9.59     0.99  
## bmi          0.09     0.02  
## age          0.03     0.01  
## ped          1.31     0.36  
## glu          0.04     0.00  
## npreg        0.12     0.04  
## bp          -0.01     0.01  
## ---
##   n = 532, k = 7
##   residual deviance = 466.5, null deviance = 676.8 (difference = 210.3)</code></pre>
<p>La <em>deviance</em> disminuye de 577 en el modelo anterior a 466.5 en este modelo, muy por encima de los 4 puntos que debería bajar simplemente al incluir 4 predictores adicionales. Además, el LRT nos indica que estas diferencias son estadísticamente significativas:</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="regresión-logística.html#cb345-1"></a><span class="kw">lrtest</span>(bin_model1, bin_model2)</span></code></pre></div>
<pre><code>## 
## Model 1: type ~ bmi + age
## Model 2: type ~ bmi + age + ped + glu + npreg + bp
## 
## L.R. Chisq       d.f.          P 
##   110.6664     4.0000     0.0000</code></pre>
<p>El segundo modelo es mucho mejor que el primero, lo que también es evidente cuando observamos las matrices de confusión (con el umbral de decisión establecido en .5)</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="regresión-logística.html#cb347-1"></a>preds &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">ifelse</span>(<span class="kw">fitted</span>(bin_model1) <span class="op">&gt;</span><span class="st"> </span><span class="fl">.5</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>))</span>
<span id="cb347-2"><a href="regresión-logística.html#cb347-2"></a><span class="kw">confusionMatrix</span>(preds, d<span class="op">$</span>type)<span class="op">$</span>table</span></code></pre></div>
<pre><code>##           Reference
## Prediction  No Yes
##        No  312 112
##        Yes  43  65</code></pre>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="regresión-logística.html#cb349-1"></a>preds &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">ifelse</span>(<span class="kw">fitted</span>(bin_model2) <span class="op">&gt;</span><span class="st"> </span><span class="fl">.5</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>))</span>
<span id="cb349-2"><a href="regresión-logística.html#cb349-2"></a><span class="kw">confusionMatrix</span>(preds, d<span class="op">$</span>type)<span class="op">$</span>table</span></code></pre></div>
<pre><code>##           Reference
## Prediction  No Yes
##        No  318  73
##        Yes  37 104</code></pre>
<p>Como era de esperar, el modelo completo comete menos errores. Podemos formalizar esta impresión calculando y comparando la precisión del modelo: 1 - (112 + 43) / (112 + 43 + 312 + 65) = 0.71 para el primer modelo, en comparación con 1 - (73 + 37) / (73 + 37 + 318 + 104) =<code>r round (1 - (73 + 37) / (73 + 37 + 318 + 104 ), 2)</code> para el segundo modelo más grande. Los números de verdaderos negativos son cercanos, pero el modelo más grande aumenta sustancialmente el número de verdaderos positivos y reduce el número de falsos negativos, prediciendo incorrectamente que una persona no tiene diabetes (aunque esta sigue siendo la clase de error más grande). Deberíamos comprobar para ver que estos modelos mejoran la precisión sobre un modelo que siempre predice la clase mayoritaria. En este conjunto de datos, “No” es la categoría mayoritaria con 66,7%. Entonces, si siempre predijimos “No”, estaríamos en lo correcto el 66.7% de las veces, que es una precisión menor que cualquiera de los modelos. Las curvas ROC proporcionan una descripción más sistemática del rendimiento del modelo en términos de errores de clasificación errónea.</p>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="regresión-logística.html#cb351-1"></a><span class="kw">invisible</span>(<span class="kw">plot</span>(<span class="kw">roc</span>(d<span class="op">$</span>type,</span>
<span id="cb351-2"><a href="regresión-logística.html#cb351-2"></a>                   <span class="kw">fitted</span>(bin_model1)),</span>
<span id="cb351-3"><a href="regresión-logística.html#cb351-3"></a>               <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, </span>
<span id="cb351-4"><a href="regresión-logística.html#cb351-4"></a>               <span class="dt">main =</span> <span class="st">&quot;ROC curves: logistic model 1 (red) vs. logistic model 2 (blue)&quot;</span>))</span>
<span id="cb351-5"><a href="regresión-logística.html#cb351-5"></a><span class="kw">invisible</span>(<span class="kw">plot</span>(<span class="kw">roc</span>(d<span class="op">$</span>type,</span>
<span id="cb351-6"><a href="regresión-logística.html#cb351-6"></a>                   <span class="kw">fitted</span>(bin_model2)),</span>
<span id="cb351-7"><a href="regresión-logística.html#cb351-7"></a>               <span class="dt">print.auc =</span> T, </span>
<span id="cb351-8"><a href="regresión-logística.html#cb351-8"></a>               <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>, </span>
<span id="cb351-9"><a href="regresión-logística.html#cb351-9"></a>               <span class="dt">add =</span> T))</span></code></pre></div>
<p><img src="fig/unnamed-chunk-114-1.png" width="672" /></p>
<p>El modelo más grande es claramente mejor: el AUC es más alto.</p>
</div>
<div id="análisis-de-interacciones" class="section level3" number="6.6.3">
<h3><span class="header-section-number">6.6.3</span> Análisis de interacciones</h3>
<p>Agreguemos una interacción entre dos predictores continuos, ped y glu. Centraremos y escalaremos las entradas para que el coeficiente de la interacción sea interpretable.</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="regresión-logística.html#cb352-1"></a>bin_model3 &lt;-<span class="st"> </span></span>
<span id="cb352-2"><a href="regresión-logística.html#cb352-2"></a><span class="st">  </span><span class="kw">standardize</span>(<span class="kw">glm</span>(type <span class="op">~</span><span class="st"> </span>bmi  <span class="op">+</span><span class="st"> </span>ped <span class="op">*</span><span class="st"> </span>glu <span class="op">+</span><span class="st">  </span>age <span class="op">+</span><span class="st"> </span>npreg <span class="op">+</span><span class="st"> </span>bp , </span>
<span id="cb352-3"><a href="regresión-logística.html#cb352-3"></a>      <span class="dt">data =</span> d, </span>
<span id="cb352-4"><a href="regresión-logística.html#cb352-4"></a>      <span class="dt">family =</span> binomial)) </span>
<span id="cb352-5"><a href="regresión-logística.html#cb352-5"></a><span class="kw">display</span>(bin_model3)</span></code></pre></div>
<pre><code>## glm(formula = type ~ z.bmi + z.ped * z.glu + z.age + z.npreg + 
##     z.bp, family = binomial, data = d)
##             coef.est coef.se
## (Intercept) -1.02     0.13  
## z.bmi        1.29     0.26  
## z.ped        1.02     0.24  
## z.glu        2.31     0.27  
## z.age        0.53     0.30  
## z.npreg      0.87     0.29  
## z.bp        -0.19     0.26  
## z.ped:z.glu -1.15     0.41  
## ---
##   n = 532, k = 8
##   residual deviance = 460.0, null deviance = 676.8 (difference = 216.7)</code></pre>
<p>La interacción mejora el modelo pero no cambia la imagen general del ajuste del modelo en el gráfico de residuos agrupados (no incluido).</p>
<ul>
<li><em>Intercept </em>: -1.02 es el <em>log-odds</em> de diabetes cuando todos los predictores son promedio (ya que hemos centrado y escalado las entradas). La probabilidad de tener diabetes para la mujer promedio en este conjunto de datos es logit<span class="math inline">\(^{- 1}\)</span> (- 1.02) = 0.27.</li>
<li><em>z.bmi</em>: 1.29 es el <em>log-odds</em> de diabetes asociado con un aumento de 1 unidad en z.bmi, manteniendo constantes los demás predictores. <span class="math inline">\(e^{1.29}\)</span> = 3.63 por lo que un aumento de 1 unidad en z.bmi, manteniendo constantes los otros predictores, se asocia con un aumento del 263% en la probabilidad de sufrir diabetes.</li>
<li><em>z.ped</em>: 1.02 es el <em>log-odds</em> de diabetes asociado con un aumento de 1 unidad en z.ped, cuando z.glu = 0 y manteniendo constantes los otros predictores. <span class="math inline">\(e^{1.02}\)</span> = 2.77 por lo que un aumento de 1 unidad en z.ped, cuando z.glu = 0 y manteniendo los otros predictores constantes, se asocia con un aumento del 177% en la probabilidad de sufrir diabetes.</li>
<li><em>z.glu</em>: 2.31 es el <em>log-odds</em> de diabetes asociado con un aumento de 1 unidad en z.glu, cuando z.ped = 0 y manteniendo constantes los demás predictores. <span class="math inline">\(e^{2.31}\)</span> = 10.07 por lo que un aumento de 1 unidad en z.glu, cuando z.ped = 0 y manteniendo los otros predictores constantes, se asocia con un aumento del 907% en la probabilidad de sufrir diabetes.</li>
<li>…. el resto de predictores igual</li>
<li></li>
<li><em>z.ped:z.glu </em>: se añade -1.15 al <em>log-odds</em> de diabetes de z.ped cuando z.glu aumenta en 1 unidad, manteniendo constantes los otros predictores. O, alternativamente, se añade -1.15 al <em>log-odds</em> de diabetes de z.glu por cada unidad adicional de z.ped. Calculamos el OR, como en los otros casos, exponenciando: <span class="math inline">\(e^ {- 1.15}\)</span> = 0.32. El OR para z.ped disminuye en un 68% (1 - .32 = .68) cuando z.glu aumenta en 1 unidad, manteniendo constantes los demás predictores. O, alternativamente, el OR para z.glu disminuye en un 68% con cada unidad adicional de z.ped.</li>
</ul>
</div>
<div id="gráfico-de-la-interacción" class="section level3" number="6.6.4">
<h3><span class="header-section-number">6.6.4</span> Gráfico de la interacción</h3>
<p>Como siempre, debemos visualizar la interacción para comprenderla. Esto es especialmente necesario cuando las relaciones se expresan en términos de probabilidades logarítmicas y razones de probabilidades. Como hemos hecho anteriormente para fines de visualización, dicotomizaremos los predictores en la interacción y, en este caso, para facilitar la interpretación, presentaremos las relaciones en términos de probabilidades. El propósito de los gráficos es la comprensión y la ilustración, por lo que no nos preocupa demasiado la precisión estadística. Resumiremos las relaciones usando una curva loess (estimación no paramétrica de la regresión) para capturar la no linealidad del efecto del predictor (<span class="math inline">\(\pm \infty\)</span>) al rango del resultado binario (0, 1).</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="regresión-logística.html#cb354-1"></a>d<span class="op">$</span>ped_bin &lt;-<span class="st"> </span><span class="kw">ifelse</span>(d<span class="op">$</span>ped <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(d<span class="op">$</span>ped), <span class="st">&quot;above avg&quot;</span>,<span class="st">&quot;below avg&quot;</span>)</span>
<span id="cb354-2"><a href="regresión-logística.html#cb354-2"></a>d<span class="op">$</span>glu_bin &lt;-<span class="st"> </span><span class="kw">ifelse</span>(d<span class="op">$</span>glu <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(d<span class="op">$</span>glu), <span class="st">&quot;above avg&quot;</span>,<span class="st">&quot;below avg&quot;</span>)</span>
<span id="cb354-3"><a href="regresión-logística.html#cb354-3"></a>d<span class="op">$</span>prob &lt;-<span class="st"> </span><span class="kw">fitted</span>(bin_model3)</span>
<span id="cb354-4"><a href="regresión-logística.html#cb354-4"></a>d<span class="op">$</span>type_num &lt;-<span class="st"> </span><span class="kw">ifelse</span>(d<span class="op">$</span>type <span class="op">==</span><span class="st"> &quot;Yes&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb354-5"><a href="regresión-logística.html#cb354-5"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(glu, type_num)) <span class="op">+</span></span>
<span id="cb354-6"><a href="regresión-logística.html#cb354-6"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb354-7"><a href="regresión-logística.html#cb354-7"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="kw">aes</span>(glu, prob, <span class="dt">col =</span> ped_bin), <span class="dt">se =</span> F) <span class="op">+</span></span>
<span id="cb354-8"><a href="regresión-logística.html#cb354-8"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;Pr(diabetes)&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Diabetes ~ glu varying by ped_bin&quot;</span>)</span></code></pre></div>
<p><img src="fig/unnamed-chunk-116-1.png" width="672" /></p>
<p>La relación entre glu y diabetes depende claramente de los niveles de ped. Como en el caso lineal, las líneas no paralelas indican una interacción. El coeficiente de <em>log-odds</em> negativo para la interacción del modelo indica que a medida que ped aumenta, la fuerza de la relación entre glu y type (diabetes) disminuye. Esto es exactamente lo que vemos en este gráfico:</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="regresión-logística.html#cb355-1"></a><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(ped, type_num)) <span class="op">+</span></span>
<span id="cb355-2"><a href="regresión-logística.html#cb355-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb355-3"><a href="regresión-logística.html#cb355-3"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="kw">aes</span>(ped, prob, <span class="dt">col =</span> glu_bin), <span class="dt">se =</span> F) <span class="op">+</span></span>
<span id="cb355-4"><a href="regresión-logística.html#cb355-4"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;Pr(diabetes)&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Diabetes ~ ped varying by glu_bin&quot;</span>)</span></code></pre></div>
<p><img src="fig/unnamed-chunk-117-1.png" width="672" /></p>
<p>La interacción es más difícil de ver aquí porque la escala de ped está comprimida, con la mayoría de las observaciones cercanas a 0. Sin embargo, podemos ver que a medida que glu aumenta, la fuerza de la relación entre ped y diabetes disminuye. Nuevamente, las líneas no paralelas indican la presencia de una interacción.</p>
</div>
<div id="uso-del-modelo-para-predecir-probabilidades" class="section level3" number="6.6.5">
<h3><span class="header-section-number">6.6.5</span> Uso del modelo para predecir probabilidades</h3>
<p>El tamaño del efecto más grande en el modelo anterior con la interacción, con mucho, es z.glu. Por tanto, para comunicar los resultados de este modelo deberíamos concentrarnos en glu. Pero los coeficientes expresados como logaritmos de probabilidades son algo confusos y, lamentablemente, las razones de probabilidades no ayudan a aclarar mucho las cosas. Deberíamos ir al trabajo adicional de traducir los coeficientes del modelo en probabilidades, pero para hacerlo debemos identificar en qué parte de la curva de probabilidad nos gustaría evaluar glu. Escogeremos la región cercana al promedio de z.glu — 0 — y examinaremos el efecto de aumentar z.glu en 1 unidad (que es igual a 2 desviaciones estándar de glu) cuando los otros predictores son promedio. La forma más sencilla de hacer esto es crear una base de datos con los valores de predicción deseados para usar con la función <code>predict ()</code>.</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="regresión-logística.html#cb356-1"></a>basal &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">z.bmi =</span> <span class="dv">0</span>, <span class="dt">z.ped =</span> <span class="dv">0</span>, <span class="dt">z.glu =</span> <span class="dv">0</span>, <span class="dt">z.age =</span> <span class="dv">0</span>, <span class="dt">z.npreg =</span> <span class="dv">0</span>, <span class="dt">z.bp =</span> <span class="dv">0</span>)</span>
<span id="cb356-2"><a href="regresión-logística.html#cb356-2"></a>glucosa &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">z.bmi =</span> <span class="dv">0</span>, <span class="dt">z.ped =</span> <span class="dv">0</span>, <span class="dt">z.glu =</span> <span class="dv">1</span>, <span class="dt">z.age =</span> <span class="dv">0</span>, <span class="dt">z.npreg =</span> <span class="dv">0</span>, <span class="dt">z.bp =</span> <span class="dv">0</span>)</span>
<span id="cb356-3"><a href="regresión-logística.html#cb356-3"></a>(lo &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">invlogit</span>(<span class="kw">predict</span>(bin_model3, <span class="dt">newdata =</span> basal))))</span></code></pre></div>
<pre><code>## [1] 0.2652028</code></pre>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="regresión-logística.html#cb358-1"></a>(hi &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">invlogit</span>(<span class="kw">predict</span>(bin_model3, <span class="dt">newdata =</span> ))))</span></code></pre></div>
<pre><code>##   [1] 0.716146591 0.025911688 0.017983270 0.029013744 0.893018807 0.665405710 0.395033985 0.211066662 0.431695033 0.225375587
##  [11] 0.032840692 0.443676401 0.022607120 0.418795847 0.194085484 0.411388311 0.122928759 0.771984610 0.760725696 0.088470567
##  [21] 0.953485181 0.785645203 0.028409690 0.026232726 0.055457646 0.038181372 0.757850071 0.007630220 0.110507313 0.066603557
##  [31] 0.213559028 0.017048355 0.294579371 0.213686248 0.245378175 0.147939308 0.058529536 0.047092679 0.169073996 0.023421517
##  [41] 0.434375748 0.123923484 0.796680824 0.257623355 0.236877492 0.032937565 0.075796893 0.459769771 0.014969537 0.214416331
##  [51] 0.409898562 0.024597682 0.736084480 0.071421984 0.863089942 0.109813203 0.415148971 0.192888308 0.671705524 0.707181753
##  [61] 0.222687051 0.074378276 0.047288218 0.250211485 0.102873418 0.451885105 0.016412055 0.839500999 0.593725195 0.887724350
##  [71] 0.023636511 0.972427462 0.421875342 0.240818110 0.193220347 0.275433738 0.029285367 0.858427986 0.792694816 0.428056807
##  [81] 0.241389504 0.359146427 0.159848949 0.126229936 0.089106804 0.965646716 0.021565093 0.504101659 0.255700597 0.303575110
##  [91] 0.316087819 0.695623695 0.059157940 0.023850831 0.703188586 0.768676985 0.198583192 0.736445668 0.019832290 0.911158959
## [101] 0.794437117 0.023391174 0.082335119 0.536540158 0.491636172 0.875915114 0.830550408 0.453101254 0.050024142 0.017749687
## [111] 0.151803493 0.175667620 0.900329614 0.125898408 0.410586667 0.586540636 0.832688763 0.035561910 0.252327286 0.110864806
## [121] 0.054988088 0.131662542 0.393097130 0.492496368 0.513182966 0.032671230 0.036078371 0.218212971 0.686096496 0.281975363
## [131] 0.145884637 0.351411775 0.239547073 0.725330877 0.392954359 0.354585338 0.328110094 0.183872054 0.649535926 0.189835213
## [141] 0.750660386 0.100154335 0.253081898 0.306902095 0.138876568 0.112315917 0.602945913 0.033225614 0.021023766 0.126008933
## [151] 0.074805807 0.593242488 0.276438109 0.030025810 0.388107478 0.814468072 0.819988723 0.320406177 0.512019175 0.189866084
## [161] 0.019102564 0.778045038 0.063532324 0.021660839 0.087123658 0.371697775 0.017831740 0.174573799 0.072586081 0.050795966
## [171] 0.321374939 0.390300727 0.242283766 0.101281785 0.477098938 0.177832066 0.851704388 0.421578801 0.059594070 0.801660505
## [181] 0.263016503 0.158261796 0.629712769 0.680147379 0.627289909 0.143229332 0.371021820 0.048405335 0.155912833 0.844592024
## [191] 0.774738072 0.074975116 0.094313006 0.034029316 0.156403796 0.765185930 0.162409946 0.941491860 0.098203334 0.096782855
## [201] 0.013696456 0.086137707 0.665930779 0.328426313 0.230933621 0.045364772 0.067599667 0.128835144 0.552086895 0.192049128
## [211] 0.391808734 0.634609576 0.138796509 0.034537822 0.402157076 0.473285370 0.915578006 0.109882027 0.060813059 0.078347861
## [221] 0.511021110 0.046236210 0.637413042 0.057352160 0.060578365 0.328120324 0.117022441 0.150080125 0.033383541 0.548421342
## [231] 0.727231653 0.273012605 0.006665104 0.023959522 0.212622505 0.263913260 0.304803089 0.282446484 0.528587177 0.053303093
## [241] 0.042787259 0.895978588 0.951451693 0.260631793 0.057999466 0.077448786 0.039445750 0.078320973 0.521759602 0.120793953
## [251] 0.109749728 0.115212481 0.063160685 0.315382778 0.186348273 0.242141093 0.800887574 0.866564275 0.147824356 0.483279794
## [261] 0.458255040 0.058388003 0.052876258 0.244883311 0.761115562 0.021394588 0.531937078 0.751371443 0.307274782 0.801133302
## [271] 0.005240345 0.656336098 0.109766806 0.101435208 0.023548802 0.056804667 0.081720211 0.513341731 0.014456472 0.127221754
## [281] 0.744369844 0.498650317 0.182496315 0.605638441 0.021059011 0.094691510 0.633177077 0.189201907 0.229657632 0.818999683
## [291] 0.081933947 0.793780642 0.798845854 0.137588419 0.275762056 0.057395933 0.445742740 0.584089416 0.070998635 0.209775098
## [301] 0.174014948 0.294145785 0.743857124 0.126042513 0.893349297 0.712229694 0.217225821 0.135596074 0.343239883 0.187767754
## [311] 0.233598292 0.822021476 0.082764167 0.102210424 0.113711375 0.122461316 0.828791325 0.107099346 0.054386091 0.931938668
## [321] 0.326454054 0.678618259 0.863704140 0.217542759 0.056249096 0.816489720 0.491806299 0.084796292 0.943486215 0.302404878
## [331] 0.132747361 0.037305147 0.053261653 0.902996173 0.042527808 0.787771463 0.031525411 0.220902831 0.054775507 0.642266770
## [341] 0.520264613 0.707148463 0.902568015 0.695527581 0.848650675 0.466017086 0.291730904 0.027601848 0.133315806 0.771592655
## [351] 0.383831136 0.090423918 0.020481869 0.115239515 0.038125985 0.017056834 0.070184592 0.755542001 0.050653582 0.481007350
## [361] 0.195086963 0.187977925 0.169394579 0.377212811 0.151859655 0.475032680 0.186324471 0.612248541 0.023804839 0.007054577
## [371] 0.085213924 0.171343641 0.900710787 0.150595032 0.257766676 0.013926514 0.035750449 0.185105131 0.253500436 0.369424730
## [381] 0.640522481 0.841160836 0.060643162 0.051633327 0.731126069 0.154040714 0.063202801 0.337255163 0.087712142 0.059196122
## [391] 0.446522479 0.839581017 0.899946965 0.051162921 0.569809764 0.085461942 0.080315826 0.343678259 0.504516436 0.092545675
## [401] 0.175360179 0.310167781 0.720652085 0.094860697 0.905656358 0.432122047 0.742494468 0.704660222 0.065240504 0.044936864
## [411] 0.086384200 0.460776518 0.058306508 0.538740077 0.360529087 0.413937339 0.248787673 0.063168325 0.205549345 0.143823024
## [421] 0.121451790 0.204643255 0.028047843 0.262047997 0.797845942 0.020192814 0.067782381 0.880235920 0.067820578 0.093376849
## [431] 0.145058897 0.885115279 0.023833728 0.172919691 0.023186441 0.343149266 0.491733925 0.334336420 0.182785009 0.107602309
## [441] 0.159470782 0.480057756 0.718226410 0.110307283 0.282105476 0.504892387 0.023457816 0.246497165 0.288819782 0.406440777
## [451] 0.333499471 0.789228958 0.024672867 0.158285199 0.915532293 0.083660243 0.616972917 0.156244026 0.066104438 0.117576505
## [461] 0.582609228 0.935907656 0.494078320 0.906231057 0.044752423 0.098120327 0.817658808 0.063067848 0.048103513 0.094446457
## [471] 0.009656172 0.457728456 0.371287692 0.732871561 0.239173153 0.048538537 0.427026826 0.838680152 0.005916331 0.639747396
## [481] 0.192937289 0.044098532 0.307301900 0.771646053 0.893235289 0.633989608 0.325705459 0.861507987 0.924405484 0.211878609
## [491] 0.232857992 0.553168378 0.199630964 0.167297598 0.636698113 0.102745817 0.412260537 0.037681077 0.499347951 0.561847838
## [501] 0.108622049 0.036902563 0.275679034 0.142289294 0.929821372 0.853111736 0.052948764 0.186298288 0.034369934 0.468239290
## [511] 0.021331580 0.617803456 0.083865824 0.751272138 0.115022996 0.496356896 0.458485256 0.303064449 0.512265744 0.935001789
## [521] 0.011659745 0.243551235 0.234201771 0.733582682 0.439827183 0.132566837 0.200183137 0.254534757 0.620459940 0.187218540
## [531] 0.128750723 0.801532793</code></pre>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="regresión-logística.html#cb360-1"></a><span class="kw">round</span>(hi <span class="op">-</span><span class="st"> </span>lo, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##   [1]  0.45 -0.24 -0.25 -0.24  0.63  0.40  0.13 -0.05  0.17 -0.04 -0.23  0.18 -0.24  0.15 -0.07  0.15 -0.14  0.51  0.50 -0.18
##  [21]  0.69  0.52 -0.24 -0.24 -0.21 -0.23  0.49 -0.26 -0.15 -0.20 -0.05 -0.25  0.03 -0.05 -0.02 -0.12 -0.21 -0.22 -0.10 -0.24
##  [41]  0.17 -0.14  0.53 -0.01 -0.03 -0.23 -0.19  0.19 -0.25 -0.05  0.14 -0.24  0.47 -0.19  0.60 -0.16  0.15 -0.07  0.41  0.44
##  [61] -0.04 -0.19 -0.22 -0.01 -0.16  0.19 -0.25  0.57  0.33  0.62 -0.24  0.71  0.16 -0.02 -0.07  0.01 -0.24  0.59  0.53  0.16
##  [81] -0.02  0.09 -0.11 -0.14 -0.18  0.70 -0.24  0.24 -0.01  0.04  0.05  0.43 -0.21 -0.24  0.44  0.50 -0.07  0.47 -0.25  0.65
## [101]  0.53 -0.24 -0.18  0.27  0.23  0.61  0.57  0.19 -0.22 -0.25 -0.11 -0.09  0.64 -0.14  0.15  0.32  0.57 -0.23 -0.01 -0.15
## [121] -0.21 -0.13  0.13  0.23  0.25 -0.23 -0.23 -0.05  0.42  0.02 -0.12  0.09 -0.03  0.46  0.13  0.09  0.06 -0.08  0.38 -0.08
## [141]  0.49 -0.17 -0.01  0.04 -0.13 -0.15  0.34 -0.23 -0.24 -0.14 -0.19  0.33  0.01 -0.24  0.12  0.55  0.55  0.06  0.25 -0.08
## [161] -0.25  0.51 -0.20 -0.24 -0.18  0.11 -0.25 -0.09 -0.19 -0.21  0.06  0.13 -0.02 -0.16  0.21 -0.09  0.59  0.16 -0.21  0.54
## [181]  0.00 -0.11  0.36  0.41  0.36 -0.12  0.11 -0.22 -0.11  0.58  0.51 -0.19 -0.17 -0.23 -0.11  0.50 -0.10  0.68 -0.17 -0.17
## [201] -0.25 -0.18  0.40  0.06 -0.03 -0.22 -0.20 -0.14  0.29 -0.07  0.13  0.37 -0.13 -0.23  0.14  0.21  0.65 -0.16 -0.20 -0.19
## [221]  0.25 -0.22  0.37 -0.21 -0.20  0.06 -0.15 -0.12 -0.23  0.28  0.46  0.01 -0.26 -0.24 -0.05  0.00  0.04  0.02  0.26 -0.21
## [241] -0.22  0.63  0.69  0.00 -0.21 -0.19 -0.23 -0.19  0.26 -0.14 -0.16 -0.15 -0.20  0.05 -0.08 -0.02  0.54  0.60 -0.12  0.22
## [261]  0.19 -0.21 -0.21 -0.02  0.50 -0.24  0.27  0.49  0.04  0.54 -0.26  0.39 -0.16 -0.16 -0.24 -0.21 -0.18  0.25 -0.25 -0.14
## [281]  0.48  0.23 -0.08  0.34 -0.24 -0.17  0.37 -0.08 -0.04  0.55 -0.18  0.53  0.53 -0.13  0.01 -0.21  0.18  0.32 -0.19 -0.06
## [301] -0.09  0.03  0.48 -0.14  0.63  0.45 -0.05 -0.13  0.08 -0.08 -0.03  0.56 -0.18 -0.16 -0.15 -0.14  0.56 -0.16 -0.21  0.67
## [321]  0.06  0.41  0.60 -0.05 -0.21  0.55  0.23 -0.18  0.68  0.04 -0.13 -0.23 -0.21  0.64 -0.22  0.52 -0.23 -0.04 -0.21  0.38
## [341]  0.26  0.44  0.64  0.43  0.58  0.20  0.03 -0.24 -0.13  0.51  0.12 -0.17 -0.24 -0.15 -0.23 -0.25 -0.20  0.49 -0.21  0.22
## [361] -0.07 -0.08 -0.10  0.11 -0.11  0.21 -0.08  0.35 -0.24 -0.26 -0.18 -0.09  0.64 -0.11 -0.01 -0.25 -0.23 -0.08 -0.01  0.10
## [381]  0.38  0.58 -0.20 -0.21  0.47 -0.11 -0.20  0.07 -0.18 -0.21  0.18  0.57  0.63 -0.21  0.30 -0.18 -0.18  0.08  0.24 -0.17
## [401] -0.09  0.04  0.46 -0.17  0.64  0.17  0.48  0.44 -0.20 -0.22 -0.18  0.20 -0.21  0.27  0.10  0.15 -0.02 -0.20 -0.06 -0.12
## [421] -0.14 -0.06 -0.24  0.00  0.53 -0.25 -0.20  0.62 -0.20 -0.17 -0.12  0.62 -0.24 -0.09 -0.24  0.08  0.23  0.07 -0.08 -0.16
## [441] -0.11  0.21  0.45 -0.15  0.02  0.24 -0.24 -0.02  0.02  0.14  0.07  0.52 -0.24 -0.11  0.65 -0.18  0.35 -0.11 -0.20 -0.15
## [461]  0.32  0.67  0.23  0.64 -0.22 -0.17  0.55 -0.20 -0.22 -0.17 -0.26  0.19  0.11  0.47 -0.03 -0.22  0.16  0.57 -0.26  0.37
## [481] -0.07 -0.22  0.04  0.51  0.63  0.37  0.06  0.60  0.66 -0.05 -0.03  0.29 -0.07 -0.10  0.37 -0.16  0.15 -0.23  0.23  0.30
## [501] -0.16 -0.23  0.01 -0.12  0.66  0.59 -0.21 -0.08 -0.23  0.20 -0.24  0.35 -0.18  0.49 -0.15  0.23  0.19  0.04  0.25  0.67
## [521] -0.25 -0.02 -0.03  0.47  0.17 -0.13 -0.07 -0.01  0.36 -0.08 -0.14  0.54</code></pre>
</div>
</div>
<div id="creación-de-un-modelo-y-validación" class="section level2" number="6.7">
<h2><span class="header-section-number">6.7</span> Creación de un modelo y validación</h2>
<p>Todo lo explicado en la sección de selección de variables para un modelo lineal, aplica para el caso de la regresión logística. Las funciones reconocen que el objeto es un glm con familia binomial y realiza los cálculos requeridos para este tipo de regresión (progamación orientada a objetos). El tema de validación cruzada para evaluar un modelo también aplica. Veámoslo con un ejemplo.</p>
<p>Supongamos que queremos elegir el mejor modelo para predecir el riesgo de diabetes y queremos validarlo con valización cruzada. Todos los pasos y métodos que hemos aprendido en las lecciones anteriores, podemos realizarlos de la siguiente manera. Usaremos los datos train y test que hay por defecto (Pima.tr y Pima.te). Para la validación cruzada usaremos la librería <code>caret</code> que veremos en detalle más adelante.</p>
<p>Empecemos seleccionando el mejor modelo en los datos de entrenamiento con un stepwise hacia atrás</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="regresión-logística.html#cb362-1"></a>mod.all &lt;-<span class="st"> </span><span class="kw">glm</span>(type <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>Pima.tr, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb362-2"><a href="regresión-logística.html#cb362-2"></a>mod &lt;-<span class="st"> </span><span class="kw">step</span>(mod.all, <span class="dt">trace=</span><span class="ot">FALSE</span>, <span class="dt">direction=</span><span class="st">&quot;backward&quot;</span>)</span>
<span id="cb362-3"><a href="regresión-logística.html#cb362-3"></a><span class="kw">summary</span>(mod)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = type ~ npreg + glu + bmi + ped + age, family = &quot;binomial&quot;, 
##     data = Pima.tr)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0009  -0.6816  -0.3664   0.6467   2.2898  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -9.938059   1.541571  -6.447 1.14e-10 ***
## npreg        0.103142   0.064517   1.599  0.10989    
## glu          0.031809   0.006667   4.771 1.83e-06 ***
## bmi          0.079672   0.032649   2.440  0.01468 *  
## ped          1.811417   0.661048   2.740  0.00614 ** 
## age          0.039286   0.020967   1.874  0.06097 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 256.41  on 199  degrees of freedom
## Residual deviance: 178.47  on 194  degrees of freedom
## AIC: 190.47
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Podemos evaluar la capacidad predictiva en la muestra test mediante</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="regresión-logística.html#cb364-1"></a>preds &lt;-<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> Pima.te, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb364-2"><a href="regresión-logística.html#cb364-2"></a>preds &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">ifelse</span>(preds <span class="op">&gt;=</span><span class="st"> </span><span class="fl">.5</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>) )</span>
<span id="cb364-3"><a href="regresión-logística.html#cb364-3"></a><span class="kw">confusionMatrix</span>(preds, Pima.te<span class="op">$</span>type)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  No Yes
##        No  199  42
##        Yes  24  67
##                                           
##                Accuracy : 0.8012          
##                  95% CI : (0.7542, 0.8428)
##     No Information Rate : 0.6717          
##     P-Value [Acc &gt; NIR] : 1.116e-07       
##                                           
##                   Kappa : 0.5294          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.03639         
##                                           
##             Sensitivity : 0.8924          
##             Specificity : 0.6147          
##          Pos Pred Value : 0.8257          
##          Neg Pred Value : 0.7363          
##              Prevalence : 0.6717          
##          Detection Rate : 0.5994          
##    Detection Prevalence : 0.7259          
##       Balanced Accuracy : 0.7535          
##                                           
##        &#39;Positive&#39; Class : No              
## </code></pre>
<p>y calcular la capacidad predictiva en la muestra train utilizando un método de 5-fold cross-validation con:</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="regresión-logística.html#cb366-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb366-2"><a href="regresión-logística.html#cb366-2"></a>mod.test &lt;-<span class="st"> </span><span class="kw">train</span>(type <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>Pima.tr, </span>
<span id="cb366-3"><a href="regresión-logística.html#cb366-3"></a>                  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number=</span><span class="dv">5</span>),</span>
<span id="cb366-4"><a href="regresión-logística.html#cb366-4"></a>                  <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb366-5"><a href="regresión-logística.html#cb366-5"></a>mod.test</span></code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 200 samples
##   7 predictor
##   2 classes: &#39;No&#39;, &#39;Yes&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 160, 160, 159, 160, 161 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.7394246  0.3961653</code></pre>
</div>
<div id="nomogramas" class="section level2" number="6.8">
<h2><span class="header-section-number">6.8</span> Nomogramas</h2>
<p>Una vez que ya tenemos creado y validad un modelo predictivo, nos puede interesar aplicarlo en otros individuos para poder tomar decisiones. Para ello, podemos usar nomogramas.</p>
<p>Un nomograma es una representación gráfica que permite realizar con rapidez cálculos numéricos aproximados. Dentro del campo de la medicina, es frecuente que este tipo de gráficos este asociado al calculo de probabilidades de ocurrencia de un evento o una característica asociada a una enfermedad. Es lo que se conoce como <strong>Medicina Translacional</strong>.</p>
<p>Aunque existen otro tipo de herramientas de cálculo vía web para estas probabilidades (Shiny), el uso de nomogramas esta muy extendido en el campo de la biomedicina como por ejemplo en el calculo de probabilidades de recurrencia en distintos tipos de cáncer, la probabilidad de supervivencia a un mes tras un infarto, o el pronóstico tras un diagnóstico de cáncer a cierto tiempo (en ese caso se usan modelos de supervivencia. Así pues, la regresión logística será una de las herramientas con una aplicación más directa y sencilla en el aprendizaje automático, donde el uso de los modelos predictivos suelen tener un aplicabilidad directa en la población.</p>
<p>Existen numerosas herramientas para crear nomogramas en R, empecemos con la creación de nomogramas sencillos. Para ello usaremos los datos del ejemplo de diabetes con el modelo que hemos obtenido y validado anteriormente. Para usar la librería <code>rms</code> necesitamos que el modelo esté estimado con la función <code>lrm ()</code></p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="regresión-logística.html#cb368-1"></a><span class="kw">library</span>(rms)</span>
<span id="cb368-2"><a href="regresión-logística.html#cb368-2"></a></span>
<span id="cb368-3"><a href="regresión-logística.html#cb368-3"></a>t.data &lt;-<span class="st"> </span><span class="kw">datadist</span>(Pima.tr)</span>
<span id="cb368-4"><a href="regresión-logística.html#cb368-4"></a><span class="kw">options</span>(<span class="dt">datadist =</span> <span class="st">&#39;t.data&#39;</span>)</span>
<span id="cb368-5"><a href="regresión-logística.html#cb368-5"></a></span>
<span id="cb368-6"><a href="regresión-logística.html#cb368-6"></a>mod.lrm &lt;-<span class="st"> </span><span class="kw">lrm</span>(type <span class="op">~</span><span class="st"> </span>npreg <span class="op">+</span><span class="st"> </span>glu <span class="op">+</span><span class="st"> </span>bmi <span class="op">+</span><span class="st"> </span>ped <span class="op">+</span><span class="st"> </span>age, <span class="dt">data=</span>Pima.tr)</span>
<span id="cb368-7"><a href="regresión-logística.html#cb368-7"></a></span>
<span id="cb368-8"><a href="regresión-logística.html#cb368-8"></a>nom &lt;-<span class="st"> </span><span class="kw">nomogram</span>(mod.lrm, <span class="dt">fun =</span> plogis, <span class="dt">funlabel=</span><span class="st">&quot;Risk of diabetes&quot;</span>)</span>
<span id="cb368-9"><a href="regresión-logística.html#cb368-9"></a><span class="kw">plot</span>(nom)</span></code></pre></div>
<p><img src="fig/unnamed-chunk-122-1.png" width="672" /></p>
<p>Supongamos que llega a la consulta una persona con un bmi de 35. Eso sumaría 30 puntos (basta con subir hacia arriba y ver qué valor de ‘Points’ corresponde a un bmi de 35). Una edad de 50 años (~22 puntos), una función del pedigrí de la diabetes de 1.8 (~62 puntos), una glucosa de 120 (~50 puntos) y 0 embarazos que sumaría 0 puntos. En total, el paciente suma un total de 164 puntos. Si ahora vamos a la línea de ‘Total Points’ y proyectamos sobre el predictor lineal de aproximadamente ~1.9 que se asocia con un riesgo de diabetes ligeramente superior al 80% (proyectar sobre ‘Risk of diabetes’).</p>
<p>Obviamente estos cálculos se pueden hacer de forma más directa calculando la predicción sobre este individuo con el objeto de R</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="regresión-logística.html#cb369-1"></a>indiv &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">bmi=</span><span class="dv">35</span>, <span class="dt">age=</span><span class="dv">50</span>, <span class="dt">ped=</span><span class="fl">1.8</span>, <span class="dt">glu=</span><span class="dv">120</span>, <span class="dt">npreg=</span><span class="dv">0</span>)</span>
<span id="cb369-2"><a href="regresión-logística.html#cb369-2"></a><span class="kw">predict</span>(mod.lrm, <span class="dt">newdata =</span> indiv, <span class="dt">type=</span><span class="st">&quot;lp&quot;</span>)</span></code></pre></div>
<pre><code>##        1 
## 1.892376</code></pre>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="regresión-logística.html#cb371-1"></a><span class="kw">predict</span>(mod.lrm, <span class="dt">newdata =</span> indiv, <span class="dt">type =</span> <span class="st">&quot;fitted&quot;</span>)</span></code></pre></div>
<pre><code>##         1 
## 0.8690262</code></pre>
<p>Estos cálculos se pueden programar en R y hace una función, o también una aplicación Shiny para aquellos médicos que no sepan usar R (los nomogramas se siguen utilizando). Otra opción es que hagamos uso de una librería para hacer nomogramas dinámicos (crea Shiny app) de forma sencilla con la librería <code>DynNom</code></p>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="regresión-logística.html#cb373-1"></a><span class="kw">library</span>(DynNom)</span>
<span id="cb373-2"><a href="regresión-logística.html#cb373-2"></a><span class="kw">DynNom</span>(bin_model2, d)</span></code></pre></div>
<p>Con estas simples instrucciones obtendríamos esta aplicación Shiny (Figura abajo) donde cada intervalo de confianza corresponde a un cálculo obtenido variando alguna de las variables predictoras</p>
<div class="figure">
<img src="figures/nomograma.png" alt="" />
<p class="caption">Nomograma para el modelo de predicción para diabetes</p>
</div>

</div>
</div>












<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>GLM significa modelo lineal generalizado. Esta función se ajustará a una variedad de modelos no lineales. Por ejemplo, podríamos especificar una regresión de Poisson con <code>family = poisson</code>.<a href="regresión-logística.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>Para obtener más detalles, se puede consultar este texto [How to interpret odds ratio in logistic regression?] (<a href="Https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/" class="uri">Https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/</a>).<a href="regresión-logística.html#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ajuste-de-modelos.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/isglobal-brge/Aprendizaje_Automatico_1/tree/master/docs05-regresion_logistica.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
